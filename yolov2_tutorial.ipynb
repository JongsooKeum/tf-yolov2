{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv2\n",
    "\n",
    "<br>\n",
    "\n",
    "### preparation\n",
    "\n",
    "- toy dataset: [FDDB: Face Detection Data Set and Benchmark](http://vis-www.cs.umass.edu/fddb/)\n",
    "\n",
    "- run the script(ellipsis_to_rectangle.py) to convert annotations in ellipsis to rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from cv2 import imread, resize, imwrite\n",
    "from matplotlib.pyplot import imshow\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_EXTENSIONS = ['png', 'jpg', 'bmp']\n",
    "\n",
    "def load_json(json_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_best_anchor(anchors, box_wh):\n",
    "    \"\"\"\n",
    "    Select the best anchor with highest IOU\n",
    "    \"\"\"\n",
    "    box_wh = np.array(box_wh)\n",
    "    best_iou = 0\n",
    "    best_anchor = 0\n",
    "    for k, anchor in enumerate(anchors):\n",
    "        intersect_wh = np.maximum(np.minimum(box_wh, anchor), 0.0)\n",
    "        intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "        box_area = box_wh[0] * box_wh[1]\n",
    "        anchor_area = anchor[0] * anchor[1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_anchor = k\n",
    "    return best_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir, image_size, pixels_per_grid=32, no_label=False):\n",
    "    \"\"\"\n",
    "    Load the data and preprocessing for YOLO detector\n",
    "    :param data_dir: str, path to the directory to read. \n",
    "                     It should include class_map, anchors, annotations\n",
    "    :image_size: tuple, image size for resizing images\n",
    "    :pixels_per_gird: int, the actual size of a grid\n",
    "    :no_label: bool, whetehr to load labels\n",
    "    :return: X_set: np.ndarray, shape: (N, H, W, C).\n",
    "             y_set: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes).\n",
    "    \"\"\"\n",
    "    im_dir = os.path.join(data_dir, 'images')\n",
    "    class_map_path = os.path.join(data_dir, 'classes.json')\n",
    "    anchors_path = os.path.join(data_dir, 'anchors.json')\n",
    "    class_map = load_json(class_map_path)\n",
    "    anchors = load_json(anchors_path)\n",
    "    num_classes = len(class_map)\n",
    "    grid_h, grid_w = [image_size[i] // pixels_per_grid for i in range(2)]\n",
    "    im_paths = []\n",
    "    for ext in IM_EXTENSIONS:\n",
    "        im_paths.extend(glob.glob(os.path.join(im_dir, '*.{}'.format(ext))))\n",
    "    anno_dir = os.path.join(data_dir, 'annotations')\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for im_path in im_paths:\n",
    "        # load image and resize image\n",
    "        im = imread(im_path)\n",
    "        im = np.array(im, dtype=np.float32)\n",
    "        im_origina_sizes = im.shape[:2]\n",
    "        im = resize(im, (image_size[1], image_size[0]))\n",
    "        if len(im.shape) == 2:\n",
    "            im = np.expand_dims(im, 2)\n",
    "            im = np.concatenate([im, im, im], -1)\n",
    "        images.append(im)\n",
    "\n",
    "        if no_label:\n",
    "            labels.append(0)\n",
    "            continue\n",
    "        # load bboxes and reshape for yolo model\n",
    "        name = os.path.splitext(os.path.basename(im_path))[0]\n",
    "        anno_path = os.path.join(anno_dir, '{}.anno'.format(name))\n",
    "        anno = load_json(anno_path)\n",
    "        label = np.zeros((grid_h, grid_w, len(anchors), 5 + num_classes))\n",
    "        for c_idx, c_name in class_map.items():\n",
    "            if c_name not in anno:\n",
    "                continue\n",
    "            for x_min, y_min, x_max, y_max in anno[c_name]:\n",
    "                oh, ow = im_origina_sizes\n",
    "                # normalize object coordinates and clip the values\n",
    "                x_min, y_min, x_max, y_max = x_min / ow, y_min / oh, x_max / ow, y_max / oh\n",
    "                x_min, y_min, x_max, y_max = np.clip([x_min, y_min, x_max, y_max], 0, 1)\n",
    "                # assign the values to the best anchor\n",
    "                anchor_boxes = np.array(anchors) / np.array([ow, oh])\n",
    "                best_anchor = get_best_anchor(\n",
    "                    anchor_boxes, [x_max - x_min, y_max - y_min])\n",
    "                cx = int(np.floor(0.5 * (x_min + x_max) * grid_w))\n",
    "                cy = int(np.floor(0.5 * (y_min + y_max) * grid_h))\n",
    "                label[cy, cx, best_anchor, 0:4] = [x_min, y_min, x_max, y_max]\n",
    "                label[cy, cx, best_anchor, 4] = 1.0\n",
    "                label[cy, cx, best_anchor, 5 + int(c_idx)] = 1.0\n",
    "        labels.append(label)\n",
    "\n",
    "    X_set = np.array(images, dtype=np.float32)\n",
    "    y_set = np.array(labels, dtype=np.float32)\n",
    "\n",
    "    return X_set, y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, images, labels=None):\n",
    "        \"\"\"\n",
    "        Construct a new DataSet object.\n",
    "        :param images: np.ndarray, shape: (N, H, W, C)\n",
    "        :param labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes).\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            assert images.shape[0] == labels.shape[0],\\\n",
    "                ('Number of examples mismatch, between images and labels')\n",
    "        self._num_examples = images.shape[0]\n",
    "        self._images = images\n",
    "        self._labels = labels  # NOTE: this can be None, if not given.\n",
    "        # image/label indices(can be permuted)\n",
    "        self._indices = np.arange(self._num_examples, dtype=np.uint)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset some variables.\"\"\"\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    def sample_batch(self, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Return sample examples from this dataset.\n",
    "        :param batch_size: int, size of a sample batch.\n",
    "        :param shuffle: bool, whether to shuffle the whole set while sampling a batch.\n",
    "        :return: batch_images: np.ndarray, shape: (N, H, W, C)\n",
    "                 batch_labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.random.choice(self._num_examples, batch_size)\n",
    "        else:\n",
    "            indices = np.arange(batch_size)\n",
    "        batch_images = self._images[indices]\n",
    "        if self._labels is not None:\n",
    "            batch_labels = self._labels[indices]\n",
    "        else:\n",
    "            batch_labels = None\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Return the next 'batch_size' examples from this dataset.\n",
    "        :param batch_size: int, size of a single batch.\n",
    "        :param shuffle: bool, whether to shuffle the whole set while sampling a batch.\n",
    "        :return: batch_images: np.ndarray, shape: (N, H, W, C)\n",
    "                 batch_labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        start_index = self._index_in_epoch\n",
    "\n",
    "        # Shuffle the dataset, for the first epoch\n",
    "        if self._epochs_completed == 0 and start_index == 0 and shuffle:\n",
    "            np.random.shuffle(self._indices)\n",
    "\n",
    "        # Go to the next epoch, if current index goes beyond the total number\n",
    "        # of examples\n",
    "        if start_index + batch_size > self._num_examples:\n",
    "            # Increment the number of epochs completed\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start_index\n",
    "            indices_rest_part = self._indices[start_index:self._num_examples]\n",
    "\n",
    "            # Shuffle the dataset, after finishing a single epoch\n",
    "            if shuffle:\n",
    "                np.random.shuffle(self._indices)\n",
    "\n",
    "            # Start the next epoch\n",
    "            start_index = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end_index = self._index_in_epoch\n",
    "            indices_new_part = self._indices[start_index:end_index]\n",
    "\n",
    "            images_rest_part = self._images[indices_rest_part]\n",
    "            images_new_part = self._images[indices_new_part]\n",
    "            batch_images = np.concatenate(\n",
    "                (images_rest_part, images_new_part), axis=0)\n",
    "            if self._labels is not None:\n",
    "                labels_rest_part = self._labels[indices_rest_part]\n",
    "                labels_new_part = self._labels[indices_new_part]\n",
    "                batch_labels = np.concatenate(\n",
    "                    (labels_rest_part, labels_new_part), axis=0)\n",
    "            else:\n",
    "                batch_labels = None\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end_index = self._index_in_epoch\n",
    "            indices = self._indices[start_index:end_index]\n",
    "            batch_images = self._images[indices]\n",
    "            if self._labels is not None:\n",
    "                batch_labels = self._labels[indices]\n",
    "            else:\n",
    "                batch_labels = None\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "root_dir = os.path.join('data/face/')\n",
    "trainval_dir = os.path.join(root_dir, 'train')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "anchors = load_json(os.path.join(trainval_dir, 'anchors.json'))\n",
    "class_map = load_json(os.path.join(trainval_dir, 'classes.json'))\n",
    "\n",
    "# set hyperparameters for data\n",
    "IM_SIZE = (416, 416)\n",
    "NUM_CLASSES = 1\n",
    "VALID_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, y_trainval = read_data(trainval_dir, IM_SIZE)\n",
    "trainval_size = X_trainval.shape[0]\n",
    "val_size = int(trainval_size * VALID_RATIO)\n",
    "val_set = DataSet(X_trainval[:val_size], y_trainval[:val_size])\n",
    "train_set = DataSet(X_trainval[val_size:], y_trainval[val_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = read_data(test_dir, IM_SIZE)\n",
    "test_set = DataSet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, filters, kernel_size, strides, padding='SAME', use_bias=True, **kwargs):\n",
    "    weights_stddev = kwargs.pop('weights_stddev', 0.01)\n",
    "    return tf.layers.conv2d(\n",
    "        x,            \n",
    "        filters,\n",
    "        kernel_size, \n",
    "        strides, \n",
    "        padding, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=weights_stddev), \n",
    "        use_bias=use_bias\n",
    "    )\n",
    "\n",
    "def batchNormalization(x, is_train):\n",
    "    \"\"\"\n",
    "    Add a new batchNormalization layer.\n",
    "    :param x: tf.Tensor, shape: (N, H, W, C) or (N, D)\n",
    "    :param is_train: tf.placeholder(bool), if True, train mode, else, test mode\n",
    "    :return: tf.Tensor.\n",
    "    \"\"\"\n",
    "    return tf.layers.batch_normalization(\n",
    "        x, \n",
    "        training=is_train, \n",
    "        momentum=0.99, \n",
    "        epsilon=0.001, \n",
    "        center=True, \n",
    "        scale=True\n",
    "    )\n",
    "\n",
    "def conv_bn_relu(x, filters, kernel_size, is_train, strides=(1, 1), padding='SAME', relu=True):\n",
    "    \"\"\"\n",
    "    Add conv + bn + Relu layers.\n",
    "    see conv_layer and batchNormalization function\n",
    "    \"\"\"\n",
    "    conv = conv_layer(x, filters, kernel_size, strides, padding, use_bias=False)\n",
    "    bn = batchNormalization(conv, is_train)\n",
    "    if relu:\n",
    "        return tf.nn.leaky_relu(bn, alpha=0.1)\n",
    "    else:\n",
    "        return bn\n",
    "\n",
    "def max_pool(x, side_l, stride, padding='SAME'):\n",
    "    \"\"\"\n",
    "    Performs max pooling on given input.\n",
    "    :param x: tf.Tensor, shape: (N, H, W, C).\n",
    "    :param side_l: int, the side length of the pooling window for each dimension.\n",
    "    :param stride: int, the stride of the sliding window for each dimension.\n",
    "    :param padding: str, either 'SAME' or 'VALID',\n",
    "                         the type of padding algorithm to use.\n",
    "    :return: tf.Tensor.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, side_l, side_l, 1],\n",
    "                          strides=[1, stride, stride, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(IM_SIZE) + [3]\n",
    "num_classes = NUM_CLASSES\n",
    "grid_size = [x // 32 for x in input_shape[:2]]\n",
    "num_anchors = len(anchors)\n",
    "\n",
    "# Prepare Input\n",
    "X = tf.placeholder(tf.float32, [None] + input_shape)\n",
    "y = tf.placeholder(tf.float32, [None] + grid_size + [num_anchors] + [5 + num_classes])\n",
    "is_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init build graph\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "\n",
    "#conv1 - batch_norm1 - leaky_relu1 - pool1\n",
    "with tf.variable_scope('layer1'):\n",
    "    d['conv1'] = conv_bn_relu(X, 32, (3, 3), is_train)\n",
    "    d['pool1'] = max_pool(d['conv1'], 2, 2, padding='SAME')\n",
    "# (416, 416, 3) --> (208, 208, 32)\n",
    "\n",
    "#conv2 - batch_norm2 - leaky_relu2 - pool2\n",
    "with tf.variable_scope('layer2'):\n",
    "    d['conv2'] = conv_bn_relu(d['pool1'], 64, (3, 3), is_train)\n",
    "    d['pool2'] = max_pool(d['conv2'], 2, 2, padding='SAME')\n",
    "# (208, 208, 32) --> (104, 104, 64)\n",
    "\n",
    "#conv3 - batch_norm3 - leaky_relu3\n",
    "with tf.variable_scope('layer3'):\n",
    "    d['conv3'] = conv_bn_relu(d['pool2'], 128, (3, 3), is_train)\n",
    "# (104, 104, 64) --> (104, 104, 128)\n",
    "\n",
    "#conv4 - batch_norm4 - leaky_relu4\n",
    "with tf.variable_scope('layer4'):\n",
    "    d['conv4'] = conv_bn_relu(d['conv3'], 64, (1, 1), is_train)\n",
    "# (104, 104, 128) --> (104, 104, 64)\n",
    "\n",
    "#conv5 - batch_norm5 - leaky_relu5 - pool5\n",
    "with tf.variable_scope('layer5'):\n",
    "    d['conv5'] = conv_bn_relu(d['conv4'], 128, (3, 3), is_train)\n",
    "    d['pool5'] = max_pool(d['conv5'], 2, 2, padding='SAME')\n",
    "# (104, 104, 64) --> (52, 52, 128)\n",
    "\n",
    "#conv6 - batch_norm6 - leaky_relu6\n",
    "with tf.variable_scope('layer6'):\n",
    "    d['conv6'] = conv_bn_relu(d['pool5'], 256, (3, 3), is_train)\n",
    "# (52, 52, 128) --> (52, 52, 256)\n",
    "\n",
    "#conv7 - batch_norm7 - leaky_relu7\n",
    "with tf.variable_scope('layer7'):\n",
    "    d['conv7'] = conv_bn_relu(d['conv6'], 128, (1, 1), is_train)\n",
    "# (52, 52, 256) --> (52, 52, 128)\n",
    "\n",
    "#conv8 - batch_norm8 - leaky_relu8 - pool8\n",
    "with tf.variable_scope('layer8'):\n",
    "    d['conv8'] = conv_bn_relu(d['conv7'], 256, (3, 3), is_train)\n",
    "    d['pool8'] = max_pool(d['conv8'], 2, 2, padding='SAME')\n",
    "# (52, 52, 128) --> (26, 26, 256)\n",
    "\n",
    "#conv9 - batch_norm9 - leaky_relu9\n",
    "with tf.variable_scope('layer9'):\n",
    "    d['conv9'] = conv_bn_relu(d['pool8'], 512, (3, 3), is_train)\n",
    "# (26, 26, 256) --> (26, 26, 512)\n",
    "\n",
    "#conv10 - batch_norm10 - leaky_relu10\n",
    "with tf.variable_scope('layer10'):\n",
    "    d['conv10'] = conv_bn_relu(d['conv9'], 256, (1, 1), is_train)\n",
    "# (26, 26, 512) --> (26, 26, 256)\n",
    "\n",
    "#conv11 - batch_norm11 - leaky_relu11\n",
    "with tf.variable_scope('layer11'):\n",
    "    d['conv11'] = conv_bn_relu(d['conv10'], 512, (3, 3), is_train)\n",
    "# (26, 26, 256) --> (26, 26, 512)\n",
    "\n",
    "#conv12 - batch_norm12 - leaky_relu12\n",
    "with tf.variable_scope('layer12'):\n",
    "    d['conv12'] = conv_bn_relu(d['conv11'], 256, (1, 1), is_train)\n",
    "# (26, 26, 512) --> (26, 26, 256)\n",
    "\n",
    "#conv13 - batch_norm13 - leaky_relu13 - pool13\n",
    "with tf.variable_scope('layer13'):\n",
    "    d['conv13'] = conv_bn_relu(d['conv12'], 512, (3, 3), is_train)\n",
    "    d['pool13'] = max_pool(d['conv13'], 2, 2, padding='SAME')\n",
    "# (26, 26, 256) --> (13, 13, 512)\n",
    "\n",
    "#conv14 - batch_norm14 - leaky_relu14\n",
    "with tf.variable_scope('layer14'):\n",
    "    d['conv14'] = conv_bn_relu(d['pool13'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 512) --> (13, 13, 1024)\n",
    "\n",
    "#conv15 - batch_norm15 - leaky_relu15\n",
    "with tf.variable_scope('layer15'):\n",
    "    d['conv15'] = conv_bn_relu(d['conv14'], 512, (1, 1), is_train)\n",
    "# (13, 13, 1024) --> (13, 13, 512)\n",
    "\n",
    "#conv16 - batch_norm16 - leaky_relu16\n",
    "with tf.variable_scope('layer16'):\n",
    "    d['conv16'] = conv_bn_relu(d['conv15'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 512) --> (13, 13, 1024)\n",
    "\n",
    "#conv17 - batch_norm16 - leaky_relu17\n",
    "with tf.variable_scope('layer17'):\n",
    "    d['conv17'] = conv_bn_relu(d['conv16'], 512, (1, 1), is_train)\n",
    "# (13, 13, 1024) --> (13, 13, 512)\n",
    "\n",
    "#conv18 - batch_norm18 - leaky_relu18\n",
    "with tf.variable_scope('layer18'):\n",
    "    d['conv18'] = conv_bn_relu(d['conv17'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 512) --> (13, 13, 1024)\n",
    "\n",
    "#conv19 - batch_norm19 - leaky_relu19\n",
    "with tf.variable_scope('layer19'):\n",
    "    d['conv19'] = conv_bn_relu(d['conv18'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 1024) --> (13, 13, 1024)\n",
    "\n",
    "#Detection Layer\n",
    "#conv20 - batch_norm20 - leaky_relu20\n",
    "with tf.variable_scope('layer20'):\n",
    "    d['conv20'] = conv_bn_relu(d['conv19'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 1024) --> (13, 13, 1024)\n",
    "\n",
    "# concatenate layer20 and layer 13 using space to depth\n",
    "with tf.variable_scope('layer21'):\n",
    "    d['skip_connection'] = conv_bn_relu(d['conv13'], 64, (1, 1), is_train)\n",
    "    d['skip_space_to_depth_x2'] = tf.space_to_depth(\n",
    "        d['skip_connection'], block_size=2)\n",
    "    d['concat21'] = tf.concat(\n",
    "        [d['skip_space_to_depth_x2'], d['conv20']], axis=-1)\n",
    "# (13, 13, 1024) --> (13, 13, 256+1024)\n",
    "\n",
    "#conv22 - batch_norm22 - leaky_relu22\n",
    "with tf.variable_scope('layer22'):\n",
    "    d['conv22'] = conv_bn_relu(d['concat21'], 1024, (3, 3), is_train)\n",
    "# (13, 13, 1280) --> (13, 13, 1024)\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    output_channel = num_anchors * (5 + num_classes)\n",
    "    d['logits'] = conv_layer(d['conv22'], output_channel, (1, 1), (1, 1),\n",
    "                            padding='SAME', use_bias=True)\n",
    "    d['pred'] = tf.reshape(\n",
    "        d['logits'], (-1, grid_size[0], grid_size[1], num_anchors, 5 + num_classes))\n",
    "# (13, 13, 1024) --> (13, 13, num_anchors , (5 + num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = d['logits']\n",
    "pred = d['pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = [5, 5, 5, 0.5, 1.0]\n",
    "\n",
    "grid_h, grid_w = grid_size\n",
    "grid_wh = np.reshape([grid_w, grid_h], [1, 1, 1, 1, 2]).astype(np.float32)\n",
    "cxcy = np.transpose([np.tile(np.arange(grid_w), grid_h), \n",
    "                     np.repeat(np.arange(grid_h), grid_h)])\n",
    "cxcy = np.reshape(cxcy, (1, grid_h, grid_w, 1, 2))\n",
    "\n",
    "txty, twth = pred[..., 0:2], pred[..., 2:4]\n",
    "confidence = tf.sigmoid(pred[..., 4:5])\n",
    "class_probs = tf.nn.softmax(pred[..., 5:], axis=-1)\\\n",
    "            if num_classes > 1 else tf.sigmoid(pred[..., 5:])\n",
    "bxby = tf.sigmoid(txty) + cxcy\n",
    "pwph = np.reshape(anchors, (1, 1, 1, num_anchors, 2)) / 32\n",
    "bwbh = tf.exp(twth) * pwph\n",
    "\n",
    "# calculating for prediction\n",
    "nxny, nwnh = bxby / grid_wh, bwbh / grid_wh\n",
    "nx1ny1, nx2ny2 = nxny - 0.5 * nwnh, nxny + 0.5 * nwnh\n",
    "pred_y = tf.concat((nx1ny1, nx2ny2, confidence, class_probs), axis=-1)\n",
    "\n",
    "# calculating IoU for metric\n",
    "num_objects = tf.reduce_sum(y[..., 4:5], axis=[1, 2, 3, 4])\n",
    "max_nx1ny1 = tf.maximum(y[..., 0:2], nx1ny1)\n",
    "min_nx2ny2 = tf.minimum(y[..., 2:4], nx2ny2)\n",
    "intersect_wh = tf.maximum(min_nx2ny2 - max_nx1ny1, 0.0)\n",
    "intersect_area = tf.reduce_prod(intersect_wh, axis=-1)\n",
    "intersect_area = tf.where(\n",
    "    tf.equal(intersect_area, 0.0), tf.zeros_like(intersect_area), intersect_area)\n",
    "gt_box_area = tf.reduce_prod(y[..., 2:4] - y[..., 0:2], axis=-1)\n",
    "box_area = tf.reduce_prod(nx2ny2 - nx1ny1, axis=-1)\n",
    "iou = tf.truediv(intersect_area, (gt_box_area + box_area - intersect_area))\n",
    "\n",
    "gt_bxby = 0.5 * (y[..., 0:2] + y[..., 2:4]) * grid_wh\n",
    "gt_bwbh = (y[..., 2:4] - y[..., 0:2]) * grid_wh\n",
    "\n",
    "resp_mask = y[..., 4:5]\n",
    "no_resp_mask = 1.0 - resp_mask\n",
    "#gt_confidence = resp_mask * tf.expand_dims(iou, axis=-1)\n",
    "gt_confidence = resp_mask\n",
    "gt_class_probs = y[..., 5:]\n",
    "\n",
    "loss_bxby = loss_weights[0] * resp_mask * tf.square(gt_bxby - bxby)\n",
    "loss_bwbh = loss_weights[1] * resp_mask * tf.square(tf.sqrt(gt_bwbh) - tf.sqrt(bwbh))\n",
    "loss_resp_conf = loss_weights[2] * resp_mask * tf.square(gt_confidence - confidence)\n",
    "loss_no_resp_conf = loss_weights[3] * no_resp_mask * tf.square(gt_confidence - confidence)\n",
    "loss_class_probs = loss_weights[4] * resp_mask * tf.square(gt_class_probs - class_probs)\n",
    "\n",
    "merged_loss = tf.concat((\n",
    "                        loss_bxby,\n",
    "                        loss_bwbh,\n",
    "                        loss_resp_conf,\n",
    "                        loss_no_resp_conf,\n",
    "                        loss_class_probs\n",
    "                        ),\n",
    "                        axis=-1)\n",
    "total_loss = tf.reduce_sum(merged_loss, axis=-1)\n",
    "total_loss = tf.reduce_mean(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.utils import convert_boxes, predict_nms_boxes, cal_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred, **kwargs):\n",
    "    \"\"\"Compute Recall for a given predicted bboxes\"\"\"\n",
    "    nms_flag = kwargs.pop('nms_flag', True)\n",
    "    if nms_flag:\n",
    "        bboxes = predict_nms_boxes(y_pred)\n",
    "    else:\n",
    "        bboxes = convert_boxes(y_pred)\n",
    "    gt_bboxes = convert_boxes(y_true)\n",
    "    score = cal_recall(gt_bboxes, bboxes)\n",
    "    return score\n",
    "\n",
    "def predict(sess, dataset, **kwargs):\n",
    "    batch_size = kwargs.pop('batch_size', 16)\n",
    "    pred_size = dataset.num_examples\n",
    "    num_steps = pred_size // batch_size\n",
    "    flag = int(bool(pred_size % batch_size))\n",
    "    # Start prediction loop\n",
    "    _y_pred = []\n",
    "    for i in range(num_steps + flag):\n",
    "        if i == num_steps and flag:\n",
    "            _batch_size = pred_size - num_steps * batch_size\n",
    "        else:\n",
    "            _batch_size = batch_size\n",
    "        X_true, _ = dataset.next_batch(_batch_size, shuffle=False)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = sess.run(pred_y, feed_dict={\n",
    "                          X: X_true, is_train: False})\n",
    "\n",
    "        _y_pred.append(y_pred)\n",
    "    _y_pred = np.concatenate(_y_pred, axis=0)\n",
    "    return _y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=graph, config=config)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "eps = 1e-3\n",
    "num_eval = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "update_vars = tf.trainable_variables()\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).\\\n",
    "                minimize(total_loss, var_list=update_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_set.num_examples\n",
    "num_steps_per_epoch = train_size // batch_size\n",
    "num_steps = num_epochs * num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\tloss: 0.085376 |Train score: 1.000000 |Eval score: 0.679752\n",
      "[epoch 1]\tloss: 0.057030 |Train score: 1.000000 |Eval score: 0.679752\n",
      "[epoch 1]\tloss: 0.045494 |Train score: 1.000000 |Eval score: 0.570248\n",
      "[epoch 1]\tloss: 0.026273 |Train score: 1.000000 |Eval score: 0.361570\n",
      "[epoch 1]\tloss: 0.021680 |Train score: 1.000000 |Eval score: 0.411157\n",
      "[epoch 1]\tloss: 0.020450 |Train score: 1.000000 |Eval score: 0.609504\n",
      "[epoch 1]\tloss: 0.013113 |Train score: 1.000000 |Eval score: 0.611570\n",
      "[epoch 1]\tloss: 0.011094 |Train score: 1.000000 |Eval score: 0.580579\n",
      "[epoch 1]\tloss: 0.012853 |Train score: 1.000000 |Eval score: 0.669421\n",
      "[epoch 2]\tloss: 0.010693 |Train score: 0.666667 |Eval score: 0.659091\n",
      "[epoch 2]\tloss: 0.009741 |Train score: 1.000000 |Eval score: 0.675620\n",
      "[epoch 2]\tloss: 0.008764 |Train score: 1.000000 |Eval score: 0.708678\n",
      "[epoch 2]\tloss: 0.015148 |Train score: 0.600000 |Eval score: 0.692149\n",
      "[epoch 2]\tloss: 0.007463 |Train score: 1.000000 |Eval score: 0.694215\n",
      "[epoch 2]\tloss: 0.013539 |Train score: 0.750000 |Eval score: 0.723140\n",
      "[epoch 2]\tloss: 0.011275 |Train score: 0.800000 |Eval score: 0.725207\n",
      "[epoch 2]\tloss: 0.004091 |Train score: 1.000000 |Eval score: 0.719008\n",
      "[epoch 2]\tloss: 0.008455 |Train score: 1.000000 |Eval score: 0.714876\n",
      "[epoch 3]\tloss: 0.007229 |Train score: 0.750000 |Eval score: 0.762397\n",
      "[epoch 3]\tloss: 0.031696 |Train score: 0.545455 |Eval score: 0.714876\n",
      "[epoch 3]\tloss: 0.012186 |Train score: 0.500000 |Eval score: 0.762397\n",
      "[epoch 3]\tloss: 0.005491 |Train score: 1.000000 |Eval score: 0.733471\n",
      "[epoch 3]\tloss: 0.003116 |Train score: 1.000000 |Eval score: 0.768595\n",
      "[epoch 3]\tloss: 0.003055 |Train score: 1.000000 |Eval score: 0.774793\n",
      "[epoch 3]\tloss: 0.003913 |Train score: 1.000000 |Eval score: 0.783058\n",
      "[epoch 3]\tloss: 0.005915 |Train score: 1.000000 |Eval score: 0.768595\n",
      "[epoch 3]\tloss: 0.004728 |Train score: 1.000000 |Eval score: 0.760331\n",
      "[epoch 4]\tloss: 0.025875 |Train score: 0.400000 |Eval score: 0.770661\n",
      "[epoch 4]\tloss: 0.005522 |Train score: 0.666667 |Eval score: 0.778926\n",
      "[epoch 4]\tloss: 0.003597 |Train score: 1.000000 |Eval score: 0.776860\n",
      "[epoch 4]\tloss: 0.002291 |Train score: 1.000000 |Eval score: 0.809917\n",
      "[epoch 4]\tloss: 0.004056 |Train score: 1.000000 |Eval score: 0.791322\n",
      "[epoch 4]\tloss: 0.009349 |Train score: 0.750000 |Eval score: 0.805785\n",
      "[epoch 4]\tloss: 0.005692 |Train score: 1.000000 |Eval score: 0.799587\n",
      "[epoch 4]\tloss: 0.010168 |Train score: 0.400000 |Eval score: 0.750000\n",
      "[epoch 4]\tloss: 0.001513 |Train score: 1.000000 |Eval score: 0.803719\n",
      "[epoch 5]\tloss: 0.003877 |Train score: 1.000000 |Eval score: 0.799587\n",
      "[epoch 5]\tloss: 0.002667 |Train score: 1.000000 |Eval score: 0.799587\n",
      "[epoch 5]\tloss: 0.008149 |Train score: 0.800000 |Eval score: 0.778926\n",
      "[epoch 5]\tloss: 0.006625 |Train score: 0.666667 |Eval score: 0.816116\n",
      "[epoch 5]\tloss: 0.003489 |Train score: 1.000000 |Eval score: 0.809917\n",
      "[epoch 5]\tloss: 0.002412 |Train score: 1.000000 |Eval score: 0.820248\n",
      "[epoch 5]\tloss: 0.005152 |Train score: 1.000000 |Eval score: 0.807851\n",
      "[epoch 5]\tloss: 0.017486 |Train score: 0.555556 |Eval score: 0.811983\n",
      "[epoch 5]\tloss: 0.001107 |Train score: 1.000000 |Eval score: 0.805785\n",
      "[epoch 5]\tloss: 0.002785 |Train score: 1.000000 |Eval score: 0.814050\n"
     ]
    }
   ],
   "source": [
    "curr_epoch = 1\n",
    "best_score = 0\n",
    "curr_score = 0\n",
    "# Start training loop\n",
    "for i in range(num_steps):\n",
    "    X_true, y_true = train_set.next_batch(batch_size, shuffle=True)\n",
    "    _, loss, y_pred = sess.run([train_op, total_loss, pred_y],\n",
    "                              feed_dict={X:X_true, y: y_true, is_train: True})\n",
    "    if (i+1) % num_eval == 0:\n",
    "        step_score = score(y_true, y_pred)\n",
    "        eval_y_pred = predict(sess, val_set)\n",
    "        eval_score = score(val_set.labels, eval_y_pred)\n",
    "        print('[epoch {}]\\tloss: {:.6f} |Train score: {:.6f} |Eval score: {:.6f}'\n",
    "      .format(curr_epoch, loss, step_score, eval_score))\n",
    "        curr_score = eval_score\n",
    "\n",
    "    if curr_score > best_score + eps:\n",
    "        best_score = curr_score\n",
    "        saver.save(sess, './yolov2.ckpt')\n",
    "        \n",
    "    if (i+1) % num_steps_per_epoch == 0:\n",
    "        curr_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test image and draw bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import draw_pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./yolov2.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './yolov2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance: 0.8310991957104558\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = predict(sess, test_set)\n",
    "test_score = score(test_set.labels, test_y_pred)\n",
    "\n",
    "print('Test performance: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_idx = np.random.choice(test_set.num_examples, 1)\n",
    "test_images = test_set.images[r_idx]\n",
    "test_pred_y = sess.run(pred_y, feed_dict={X: test_images, is_train: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = predict_nms_boxes(test_pred_y[0], conf_thres=0.1, iou_thres=0.5)\n",
    "bboxes = bboxes[np.nonzero(np.any(bboxes > 0, axis=1))]\n",
    "boxed_img = draw_pred_boxes(test_images[0], bboxes, class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imwrite('test.png', boxed_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
